{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a9d2e5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b522989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# If using system Python, this will use --break-system-packages flag\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Check if we're in a virtual environment\n",
    "in_venv = hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix)\n",
    "\n",
    "packages = [\"streamlit\", \"tensorflow\", \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"scikit-learn\", \"scipy\"]\n",
    "\n",
    "for package in packages:\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"]\n",
    "    if not in_venv:\n",
    "        cmd.append(\"--break-system-packages\")\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d18ab80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # длина окна по времени (сколько точек траектории подаём на вход)\n",
    "    seq_len: int = 20\n",
    "\n",
    "    # число признаков на одну точку (сейчас: lat, lon)\n",
    "    n_features: int = 2\n",
    "\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 1e-3\n",
    "    epochs: int = 10\n",
    "\n",
    "    # horizon_steps — на сколько шагов вперёд смотрим:\n",
    "    # войдёт ли траектория в AOI в ближайшие horizon_steps точек\n",
    "    horizon_steps: int = 10\n",
    "\n",
    "    # Максимальное количество образцов для обучения (None = использовать все)\n",
    "    max_samples: int = 1000000  # 1M samples для начала\n",
    "\n",
    "    # Размер буфера для перемешивания (меньше = меньше памяти)\n",
    "    shuffle_buffer_size: int = 100000  # 100K samples\n",
    "\n",
    "    # путь до корня Geolife (папка, где лежат подпапки пользователей и .plt файлы)\n",
    "    geolife_root: str = \"../geo-date/Data\" \n",
    "\n",
    "    # AOI — прямоугольник (min_lat, min_lon, max_lat, max_lon)\n",
    "    # Здесь пример - прямоугольник в Пекине (примерные координаты).\n",
    "    aoi_rect: Tuple[float, float, float, float] = (\n",
    "        39.90, 116.38, 39.92, 116.42\n",
    "    )\n",
    "\n",
    "\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc79228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_in_rect(lat: float, lon: float, rect: Tuple[float, float, float, float]) -> bool:\n",
    "    \"\"\"\n",
    "    Проверяет, лежит ли точка (lat, lon) внутри прямоугольника rect.\n",
    "\n",
    "    rect = (min_lat, min_lon, max_lat, max_lon)\n",
    "    \"\"\"\n",
    "    min_lat, min_lon, max_lat, max_lon = rect\n",
    "    return (min_lat <= lat <= max_lat) and (min_lon <= lon <= max_lon)\n",
    "\n",
    "\n",
    "def read_geolife_plt_file(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Читает один .plt файл Geolife и возвращает массив координат.\n",
    "\n",
    "    Формат строк в Geolife .plt (после 6 строк заголовка):\n",
    "    lat, lon, 0, altitude, days, date, time\n",
    "\n",
    "    Здесь мы берём только lat и lon.\n",
    "\n",
    "    Возвращает:\n",
    "      numpy array формы (num_points, 2) → [lat, lon]\n",
    "    \"\"\"\n",
    "    coords: List[Tuple[float, float]] = []\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # первые 6 строк — заголовок, пропускаем\n",
    "    for line in lines[6:]:\n",
    "        parts = line.strip().split(\",\")\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            lat = float(parts[0])\n",
    "            lon = float(parts[1])\n",
    "            coords.append((lat, lon))\n",
    "        except ValueError:\n",
    "            # если строка битая, просто пропускаем\n",
    "            continue\n",
    "\n",
    "    if not coords:\n",
    "        return np.empty((0, 2), dtype=\"float32\")\n",
    "\n",
    "    return np.array(coords, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def load_geolife_trajectories(root_dir: str) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Обходит папку Geolife и собирает траектории.\n",
    "\n",
    "    Структура Geolife обычно такая:\n",
    "    root_dir/\n",
    "      000/\n",
    "        Trajectory/\n",
    "          *.plt\n",
    "      001/\n",
    "        Trajectory/\n",
    "          *.plt\n",
    "      ...\n",
    "\n",
    "    На выходе получаем список траекторий, каждая — массив (num_points, 2).\n",
    "    \"\"\"\n",
    "    trajectories: List[np.ndarray] = []\n",
    "\n",
    "    for user_id in os.listdir(root_dir):\n",
    "        user_path = os.path.join(root_dir, user_id)\n",
    "        traj_dir = os.path.join(user_path, \"Trajectory\")\n",
    "        if not os.path.isdir(traj_dir):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(traj_dir):\n",
    "            if not fname.endswith(\".plt\"):\n",
    "                continue\n",
    "            path = os.path.join(traj_dir, fname)\n",
    "            coords = read_geolife_plt_file(path)\n",
    "            if coords.shape[0] >= CFG.seq_len + CFG.horizon_steps + 1:\n",
    "                trajectories.append(coords)\n",
    "\n",
    "    print(f\"Loaded {len(trajectories)} trajectories from Geolife.\")\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "def build_windows_and_labels(\n",
    "    trajectories: List[np.ndarray],\n",
    "    seq_len: int,\n",
    "    horizon_steps: int,\n",
    "    aoi_rect: Tuple[float, float, float, float],\n",
    "    max_samples: int = None\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Из списка траекторий строим обучающие окна и метки.\n",
    "\n",
    "    Для каждой траектории:\n",
    "      траектория = [p0, p1, ..., pN], p_i = (lat, lon)\n",
    "\n",
    "    Берём окна:\n",
    "      X_window = p_{i-seq_len} ... p_{i-1}\n",
    "    и смотрим вперёд:\n",
    "      future_segment = p_i ... p_{i + horizon_steps - 1}\n",
    "\n",
    "    Если future_segment хотя бы в одной точке заходит в AOI → y = 1\n",
    "    иначе → y = 0\n",
    "\n",
    "    Возвращает:\n",
    "      X: (num_samples, seq_len, 2)\n",
    "      y: (num_samples,)\n",
    "    \"\"\"\n",
    "    X_list: List[np.ndarray] = []\n",
    "    y_list: List[float] = []\n",
    "\n",
    "    for traj in trajectories:\n",
    "        num_points = traj.shape[0]\n",
    "        # i — индекс текущего \"начала\" временного окна (будем предсказывать с этого момента вперёд)\n",
    "        # окно берём [i-seq_len, i), будущее [i, i+horizon_steps)\n",
    "        for i in range(seq_len, num_points - horizon_steps):\n",
    "            # Ограничиваем количество образцов, если указано\n",
    "            if max_samples is not None and len(X_list) >= max_samples:\n",
    "                break\n",
    "            \n",
    "            past_window = traj[i - seq_len:i]              # (seq_len, 2)\n",
    "            future_segment = traj[i:i + horizon_steps]     # (horizon_steps, 2)\n",
    "\n",
    "            # метка: войдёт ли объект в AOI в ближайшее будущее\n",
    "            will_enter = any(\n",
    "                point_in_rect(float(lat), float(lon), aoi_rect)\n",
    "                for (lat, lon) in future_segment\n",
    "            )\n",
    "            label = 1.0 if will_enter else 0.0\n",
    "\n",
    "            X_list.append(past_window)\n",
    "            y_list.append(label)\n",
    "        \n",
    "        # Прерываем внешний цикл, если достигли лимита\n",
    "        if max_samples is not None and len(X_list) >= max_samples:\n",
    "            break\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\"No training samples built. \"\n",
    "                           \"Check that trajectories are long enough and root path is correct.\")\n",
    "\n",
    "    X = np.stack(X_list).astype(\"float32\")   # (num_samples, seq_len, 2)\n",
    "    y = np.array(y_list, dtype=\"float32\")    # (num_samples,)\n",
    "\n",
    "    print(f\"Built windows: X shape={X.shape}, y shape={y.shape}\")\n",
    "    if max_samples is not None and len(X_list) >= max_samples:\n",
    "        print(f\"Limited to {max_samples} samples due to max_samples setting.\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_tf_datasets(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    validation_split: float = 0.2,\n",
    "    shuffle_buffer_size: int = None\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Разбивает массивы X, y на train/val и заворачивает в tf.data.Dataset.\n",
    "    \"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    split_idx = int(num_samples * (1.0 - validation_split))\n",
    "\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    # Используем разумный размер буфера для перемешивания\n",
    "    if shuffle_buffer_size is None:\n",
    "        shuffle_buffer_size = min(CFG.shuffle_buffer_size, len(X_train))\n",
    "    else:\n",
    "        shuffle_buffer_size = min(shuffle_buffer_size, len(X_train))\n",
    "\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .shuffle(buffer_size=shuffle_buffer_size)\n",
    "        .batch(CFG.batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    val_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        .batch(CFG.batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2bc1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_gru_model(\n",
    "    seq_len: int,\n",
    "    n_features: int\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Модель:\n",
    "      Input (seq_len, n_features)\n",
    "        → Conv1D (ловит локальные паттерны движения)\n",
    "        → MaxPool (упрощает последовательность)\n",
    "        → GRU (запоминает историю)\n",
    "        → Dense → Sigmoid (вероятность входа в AOI)\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(seq_len, n_features), name=\"trajectory_window\")\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        name=\"conv1d_1\"\n",
    "    )(inputs)\n",
    "\n",
    "    x = layers.MaxPooling1D(pool_size=2, name=\"maxpool_1\")(x)\n",
    "\n",
    "    x = layers.GRU(64, name=\"gru_1\")(x)\n",
    "\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(x)\n",
    "    x = layers.Dropout(0.3, name=\"dropout_1\")(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"GeoFence_CNN_GRU\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=CFG.learning_rate),\n",
    "        loss=losses.BinaryCrossentropy(),\n",
    "        metrics=[\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e59bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model() -> tf.keras.Model:\n",
    "    # 1) Загружаем траектории\n",
    "    trajectories = load_geolife_trajectories(CFG.geolife_root)\n",
    "\n",
    "    # 2) Строим окна и метки\n",
    "    X, y = build_windows_and_labels(\n",
    "        trajectories,\n",
    "        seq_len=CFG.seq_len,\n",
    "        horizon_steps=CFG.horizon_steps,\n",
    "        aoi_rect=CFG.aoi_rect,\n",
    "        max_samples=CFG.max_samples\n",
    "    )\n",
    "\n",
    "    # 3) Train/val split\n",
    "    train_ds, val_ds = make_tf_datasets(\n",
    "        X, y, \n",
    "        validation_split=0.2,\n",
    "        shuffle_buffer_size=CFG.shuffle_buffer_size\n",
    "    )\n",
    "\n",
    "    # 4) Модель\n",
    "    model = build_cnn_gru_model(seq_len=CFG.seq_len, n_features=CFG.n_features)\n",
    "    model.summary()\n",
    "\n",
    "    # ======= CALLBACKS =======\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_geofence_model.keras\",\n",
    "        monitor=\"val_auc\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    # ==========================\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=CFG.epochs,\n",
    "        callbacks=[checkpoint_cb, reduce_lr, early_stop],\n",
    "    )\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # ===== EXPORT FOR JAVA =====\n",
    "    export_dir = \"exported_geofence_model\"\n",
    "    model.export(export_dir)\n",
    "    print(\"SavedModel exported successfully to:\", export_dir)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d951911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_window(\n",
    "    model: tf.keras.Model,\n",
    "    window: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Предсказание для одного окна траектории.\n",
    "\n",
    "    window: numpy array формы (seq_len, n_features)\n",
    "    Возвращает вероятность входа в AOI в ближайшие horizon_steps шагов.\n",
    "    \"\"\"\n",
    "    assert window.shape == (CFG.seq_len, CFG.n_features), (\n",
    "        f\"Expected window shape {(CFG.seq_len, CFG.n_features)}, \"\n",
    "        f\"got {window.shape}\"\n",
    "    )\n",
    "\n",
    "    window_batch = np.expand_dims(window, axis=0)  # (1, seq_len, n_features)\n",
    "    prob = model.predict(window_batch, verbose=0)[0, 0]\n",
    "    return float(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1138fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17678 trajectories from Geolife.\n",
      "Built windows: X shape=(1000000, 20, 2), y shape=(1000000,)\n",
      "Limited to 1000000 samples due to max_samples setting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-04 14:29:42.743896: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 128000000 exceeds 10% of free system memory.\n",
      "2025-12-04 14:29:42.903349: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 128000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GeoFence_CNN_GRU\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GeoFence_CNN_GRU\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ trajectory_window (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ trajectory_window (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m2\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ maxpool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m18,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,265</span> (90.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,265\u001b[0m (90.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,265</span> (90.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,265\u001b[0m (90.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m12489/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9951 - auc: 0.5418 - loss: 0.0284\n",
      "Epoch 1: val_auc improved from None to 0.50000, saving model to best_geofence_model.keras\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 5ms/step - accuracy: 0.9954 - auc: 0.5410 - loss: 0.0301 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0398 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m12494/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9963 - auc: 0.5285 - loss: 0.0252\n",
      "Epoch 2: val_auc did not improve from 0.50000\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 5ms/step - accuracy: 0.9956 - auc: 0.5355 - loss: 0.0294 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0398 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m12490/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9963 - auc: 0.5317 - loss: 0.0250\n",
      "Epoch 3: val_auc did not improve from 0.50000\n",
      "\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 5ms/step - accuracy: 0.9956 - auc: 0.5434 - loss: 0.0290 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0399 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9963 - auc: 0.5377 - loss: 0.0250\n",
      "Epoch 4: val_auc did not improve from 0.50000\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 5ms/step - accuracy: 0.9956 - auc: 0.5474 - loss: 0.0287 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0397 - learning_rate: 5.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m12488/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9963 - auc: 0.5347 - loss: 0.0246\n",
      "Epoch 5: val_auc did not improve from 0.50000\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 5ms/step - accuracy: 0.9956 - auc: 0.5474 - loss: 0.0287 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0396 - learning_rate: 5.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m12496/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9963 - auc: 0.5418 - loss: 0.0245\n",
      "Epoch 6: val_auc did not improve from 0.50000\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 4ms/step - accuracy: 0.9956 - auc: 0.5530 - loss: 0.0285 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0398 - learning_rate: 5.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m12497/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9963 - auc: 0.5367 - loss: 0.0246\n",
      "Epoch 7: val_auc did not improve from 0.50000\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 4ms/step - accuracy: 0.9956 - auc: 0.5529 - loss: 0.0285 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0396 - learning_rate: 2.5000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m12497/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9963 - auc: 0.5373 - loss: 0.0243\n",
      "Epoch 8: val_auc did not improve from 0.50000\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 4ms/step - accuracy: 0.9956 - auc: 0.5503 - loss: 0.0285 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0396 - learning_rate: 2.5000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m12497/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9963 - auc: 0.5282 - loss: 0.0244\n",
      "Epoch 9: val_auc did not improve from 0.50000\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 4ms/step - accuracy: 0.9956 - auc: 0.5443 - loss: 0.0285 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0396 - learning_rate: 1.2500e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9963 - auc: 0.5390 - loss: 0.0245\n",
      "Epoch 10: val_auc did not improve from 0.50000\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m12500/12500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 4ms/step - accuracy: 0.9956 - auc: 0.5529 - loss: 0.0284 - val_accuracy: 0.9934 - val_auc: 0.5000 - val_loss: 0.0396 - learning_rate: 1.2500e-04\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Training finished.\n",
      "INFO:tensorflow:Assets written to: exported_geofence_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: exported_geofence_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'exported_geofence_model'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 20, 2), dtype=tf.float32, name='trajectory_window')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  132523504992528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132523505003472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132523504995216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132523505001360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132523504995408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132523500893584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132523500894928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132523500889936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132523500893200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "SavedModel exported successfully to: exported_geofence_model\n",
      "Intrusion probability in next 10 steps: 0.047\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = train_model()\n",
    "\n",
    "    # Пример окна\n",
    "    example_window = np.random.rand(CFG.seq_len, CFG.n_features).astype(\"float32\")\n",
    "    prob = predict_one_window(model, example_window)\n",
    "    print(f\"Intrusion probability in next {CFG.horizon_steps} steps: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "525881c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trajectories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m lat_min, lon_min, lat_max, lon_max = CFG.aoi_rect\n\u001b[32m      3\u001b[39m inside_count = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrajectories\u001b[49m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.any(\n\u001b[32m      7\u001b[39m         (traj[:,\u001b[32m0\u001b[39m] >= lat_min) &\n\u001b[32m      8\u001b[39m         (traj[:,\u001b[32m0\u001b[39m] <= lat_max) &\n\u001b[32m      9\u001b[39m         (traj[:,\u001b[32m1\u001b[39m] >= lon_min) &\n\u001b[32m     10\u001b[39m         (traj[:,\u001b[32m1\u001b[39m] <= lon_max)\n\u001b[32m     11\u001b[39m     ):\n\u001b[32m     12\u001b[39m         inside_count += \u001b[32m1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'trajectories' is not defined"
     ]
    }
   ],
   "source": [
    "lat_min, lon_min, lat_max, lon_max = CFG.aoi_rect\n",
    "\n",
    "inside_count = 0\n",
    "\n",
    "for traj in trajectories:\n",
    "    if np.any(\n",
    "        (traj[:,0] >= lat_min) &\n",
    "        (traj[:,0] <= lat_max) &\n",
    "        (traj[:,1] >= lon_min) &\n",
    "        (traj[:,1] <= lon_max)\n",
    "    ):\n",
    "        inside_count += 1\n",
    "\n",
    "print(\"Trajectories that EVER touch AOI:\", inside_count)\n",
    "print(\"Total trajectories:\", len(trajectories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b96e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Установка пакетов (можно убрать, если всё уже стоит)\n",
    "# ===============================\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "in_venv = hasattr(sys, \"real_prefix\") or (\n",
    "    hasattr(sys, \"base_prefix\") and sys.base_prefix != sys.prefix\n",
    ")\n",
    "\n",
    "packages = [\n",
    "    \"streamlit\",\n",
    "    \"tensorflow\",\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"scikit-learn\",\n",
    "    \"scipy\",\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"]\n",
    "    if not in_venv:\n",
    "        cmd.append(\"--break-system-packages\")\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "# ===============================\n",
    "# 2. Импорты и конфиг\n",
    "# ===============================\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # длина окна по времени (сколько точек траектории подаём на вход)\n",
    "    seq_len: int = 20\n",
    "\n",
    "    # число признаков на одну точку (сейчас: lat, lon)\n",
    "    n_features: int = 2\n",
    "\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 1e-3\n",
    "    epochs: int = 10\n",
    "\n",
    "    # horizon_steps — на сколько шагов вперёд смотрим:\n",
    "    # войдёт ли траектория в AOI в ближайшие horizon_steps точек\n",
    "    horizon_steps: int = 10\n",
    "\n",
    "    # Максимальное количество образцов для обучения (None = использовать все)\n",
    "    max_samples: int = 1_000_000  # 1M samples для начала\n",
    "\n",
    "    # Размер буфера для перемешивания (меньше = меньше памяти)\n",
    "    shuffle_buffer_size: int = 100_000  # 100K samples\n",
    "\n",
    "    # путь до корня Geolife (папка, где лежат подпапки пользователей и .plt файлы)\n",
    "    geolife_root: str = \"../geo-date/Data\"\n",
    "\n",
    "    # AOI — прямоугольник (min_lat, min_lon, max_lat, max_lon)\n",
    "    # Прямоугольник в Пекине (примерные координаты).\n",
    "    aoi_rect: Tuple[float, float, float, float] = (39.90, 116.38, 39.92, 116.42)\n",
    "\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3. Вспомогательные функции\n",
    "# ===============================\n",
    "def point_in_rect(lat: float, lon: float, rect: Tuple[float, float, float, float]) -> bool:\n",
    "    \"\"\"\n",
    "    Проверяет, лежит ли точка (lat, lon) внутри прямоугольника rect.\n",
    "\n",
    "    rect = (min_lat, min_lon, max_lat, max_lon)\n",
    "    \"\"\"\n",
    "    min_lat, min_lon, max_lat, max_lon = rect\n",
    "    return (min_lat <= lat <= max_lat) and (min_lon <= lon <= max_lon)\n",
    "\n",
    "\n",
    "def read_geolife_plt_file(path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Читает один .plt файл Geolife и возвращает массив координат.\n",
    "\n",
    "    Формат строк в Geolife .plt (после 6 строк заголовка):\n",
    "    lat, lon, 0, altitude, days, date, time\n",
    "\n",
    "    Здесь мы берём только lat и lon.\n",
    "\n",
    "    Возвращает:\n",
    "      numpy array формы (num_points, 2) → [lat, lon]\n",
    "    \"\"\"\n",
    "    coords: List[Tuple[float, float]] = []\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # первые 6 строк — заголовок, пропускаем\n",
    "    for line in lines[6:]:\n",
    "        parts = line.strip().split(\",\")\n",
    "        if len(parts) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            lat = float(parts[0])\n",
    "            lon = float(parts[1])\n",
    "            coords.append((lat, lon))\n",
    "        except ValueError:\n",
    "            # если строка битая, просто пропускаем\n",
    "            continue\n",
    "\n",
    "    if not coords:\n",
    "        return np.empty((0, 2), dtype=\"float32\")\n",
    "\n",
    "    return np.array(coords, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def load_geolife_trajectories(root_dir: str) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Обходит папку Geolife и собирает траектории.\n",
    "\n",
    "    Структура Geolife обычно такая:\n",
    "    root_dir/\n",
    "      000/\n",
    "        Trajectory/\n",
    "          *.plt\n",
    "      001/\n",
    "        Trajectory/\n",
    "          *.plt\n",
    "      ...\n",
    "\n",
    "    На выходе получаем список траекторий, каждая — массив (num_points, 2).\n",
    "    \"\"\"\n",
    "    trajectories: List[np.ndarray] = []\n",
    "\n",
    "    for user_id in os.listdir(root_dir):\n",
    "        user_path = os.path.join(root_dir, user_id)\n",
    "        traj_dir = os.path.join(user_path, \"Trajectory\")\n",
    "        if not os.path.isdir(traj_dir):\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(tr_dir := traj_dir):\n",
    "            if not fname.endswith(\".plt\"):\n",
    "                continue\n",
    "            path = os.path.join(tr_dir, fname)\n",
    "            coords = read_geolife_plt_file(path)\n",
    "            if coords.shape[0] >= CFG.seq_len + CFG.horizon_steps + 1:\n",
    "                trajectories.append(coords)\n",
    "\n",
    "    print(f\"Loaded {len(trajectories)} trajectories from Geolife.\")\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "def build_windows_and_labels(\n",
    "    trajectories: List[np.ndarray],\n",
    "    seq_len: int,\n",
    "    horizon_steps: int,\n",
    "    aoi_rect: Tuple[float, float, float, float],\n",
    "    max_samples: int = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Из списка траекторий строим обучающие окна и метки.\n",
    "\n",
    "    Для каждой траектории:\n",
    "      траектория = [p0, p1, ..., pN], p_i = (lat, lon)\n",
    "\n",
    "    Берём окна:\n",
    "      X_window = p_{i-seq_len} ... p_{i-1}\n",
    "    и смотрим вперёд:\n",
    "      future_segment = p_i ... p_{i + horizon_steps - 1}\n",
    "\n",
    "    Если future_segment хотя бы в одной точке заходит в AOI → y = 1\n",
    "    иначе → y = 0\n",
    "\n",
    "    Возвращает:\n",
    "      X: (num_samples, seq_len, 2)\n",
    "      y: (num_samples,)\n",
    "    \"\"\"\n",
    "    X_list: List[np.ndarray] = []\n",
    "    y_list: List[float] = []\n",
    "\n",
    "    for traj in trajectories:\n",
    "        num_points = traj.shape[0]\n",
    "        for i in range(seq_len, num_points - horizon_steps):\n",
    "            if max_samples is not None and len(X_list) >= max_samples:\n",
    "                break\n",
    "\n",
    "            past_window = traj[i - seq_len : i]  # (seq_len, 2)\n",
    "            future_segment = traj[i : i + horizon_steps]  # (horizon_steps, 2)\n",
    "\n",
    "            will_enter = any(\n",
    "                point_in_rect(float(lat), float(lon), aoi_rect)\n",
    "                for (lat, lon) in future_segment\n",
    "            )\n",
    "            label = 1.0 if will_enter else 0.0\n",
    "\n",
    "            X_list.append(past_window)\n",
    "            y_list.append(label)\n",
    "\n",
    "        if max_samples is not None and len(X_list) >= max_samples:\n",
    "            break\n",
    "\n",
    "    if not X_list:\n",
    "        raise RuntimeError(\n",
    "            \"No training samples built. \"\n",
    "            \"Check that trajectories are long enough and AOI overlaps trajectories.\"\n",
    "        )\n",
    "\n",
    "    X = np.stack(X_list).astype(\"float32\")  # (num_samples, seq_len, 2)\n",
    "    y = np.array(y_list, dtype=\"float32\")  # (num_samples,)\n",
    "\n",
    "    print(f\"Built windows: X shape={X.shape}, y shape={y.shape}\")\n",
    "    if max_samples is not None and len(X_list) >= max_samples:\n",
    "        print(f\"Limited to {max_samples} samples due to max_samples setting.\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_tf_datasets(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    validation_split: float = 0.2,\n",
    "    shuffle_buffer_size: int = None,\n",
    ") -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Разбивает массивы X, y на train/val и заворачивает в tf.data.Dataset.\n",
    "    \"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    split_idx = int(num_samples * (1.0 - validation_split))\n",
    "\n",
    "    X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    if shuffle_buffer_size is None:\n",
    "        shuffle_buffer_size = min(CFG.shuffle_buffer_size, len(X_train))\n",
    "    else:\n",
    "        shuffle_buffer_size = min(shuffle_buffer_size, len(X_train))\n",
    "\n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .shuffle(buffer_size=shuffle_buffer_size)\n",
    "        .batch(CFG.batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    val_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        .batch(CFG.batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def build_cnn_gru_model(seq_len: int, n_features: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Модель:\n",
    "      Input (seq_len, n_features)\n",
    "        → Conv1D (ловит локальные паттерны движения)\n",
    "        → MaxPool (упрощает последовательность)\n",
    "        → GRU (запоминает историю)\n",
    "        → Dense → Sigmoid (вероятность входа в AOI)\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(seq_len, n_features), name=\"trajectory_window\")\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        name=\"conv1d_1\",\n",
    "    )(inputs)\n",
    "\n",
    "    x = layers.MaxPooling1D(pool_size=2, name=\"maxpool_1\")(x)\n",
    "\n",
    "    x = layers.GRU(64, name=\"gru_1\")(x)\n",
    "\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(x)\n",
    "    x = layers.Dropout(0.3, name=\"dropout_1\")(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"GeoFence_CNN_GRU\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=CFG.learning_rate),\n",
    "        loss=losses.BinaryCrossentropy(),\n",
    "        metrics=[\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 4. Аналитика датасета (AOI + баланс классов)\n",
    "# ===============================\n",
    "def analyze_dataset(trajectories: List[np.ndarray], X: np.ndarray, y: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Печатает:\n",
    "      - сколько траекторий вообще пересекают AOI\n",
    "      - сколько положительных/отрицательных меток\n",
    "    \"\"\"\n",
    "    lat_min, lon_min, lat_max, lon_max = CFG.aoi_rect\n",
    "\n",
    "    inside_traj_count = 0\n",
    "    for traj in trajectories:\n",
    "        if np.any(\n",
    "            (traj[:, 0] >= lat_min)\n",
    "            & (traj[:, 0] <= lat_max)\n",
    "            & (traj[:, 1] >= lon_min)\n",
    "            & (traj[:, 1] <= lon_max)\n",
    "        ):\n",
    "            inside_traj_count += 1\n",
    "\n",
    "    total_traj = len(trajectories)\n",
    "    pos = float(y.sum())\n",
    "    total = len(y)\n",
    "    neg = total - pos\n",
    "    ratio = pos / total if total > 0 else 0.0\n",
    "\n",
    "    print(\"\\n=== DATASET ANALYSIS ===\")\n",
    "    print(f\"Trajectories that EVER touch AOI: {inside_traj_count} / {total_traj}\")\n",
    "    print(f\"Labels: positives={pos:.0f}, negatives={neg:.0f}, ratio={ratio:.4f}\")\n",
    "    print(\"========================\\n\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 5. Тренировка\n",
    "# ===============================\n",
    "def train_model(trajectories: List[np.ndarray]) -> tf.keras.Model:\n",
    "    # 1) Строим окна и метки\n",
    "    X, y = build_windows_and_labels(\n",
    "        trajectories,\n",
    "        seq_len=CFG.seq_len,\n",
    "        horizon_steps=CFG.horizon_steps,\n",
    "        aoi_rect=CFG.aoi_rect,\n",
    "        max_samples=CFG.max_samples,\n",
    "    )\n",
    "\n",
    "    # 2) Анализ датасета (важно: баланс классов и покрытие AOI)\n",
    "    analyze_dataset(trajectories, X, y)\n",
    "\n",
    "    # 3) Train/val split\n",
    "    train_ds, val_ds = make_tf_datasets(\n",
    "        X,\n",
    "        y,\n",
    "        validation_split=0.2,\n",
    "        shuffle_buffer_size=CFG.shuffle_buffer_size,\n",
    "    )\n",
    "\n",
    "    # 4) Модель\n",
    "    model = build_cnn_gru_model(\n",
    "        seq_len=CFG.seq_len,\n",
    "        n_features=CFG.n_features,\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    # ======= CALLBACKS =======\n",
    "    checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_geofence_model.keras\",\n",
    "        monitor=\"val_auc\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    # ==========================\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=CFG.epochs,\n",
    "        callbacks=[checkpoint_cb, reduce_lr, early_stop],\n",
    "    )\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # ===== EXPORT FOR JAVA =====\n",
    "    export_dir = \"exported_geofence_model\"\n",
    "    model.export(export_dir)\n",
    "    print(\"SavedModel exported successfully to:\", export_dir)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6. Инференс для одного окна\n",
    "# ===============================\n",
    "def predict_one_window(model: tf.keras.Model, window: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Предсказание для одного окна траектории.\n",
    "\n",
    "    window: numpy array формы (seq_len, n_features)\n",
    "    Возвращает вероятность входа в AOI в ближайшие horizon_steps шагов.\n",
    "    \"\"\"\n",
    "    assert window.shape == (CFG.seq_len, CFG.n_features), (\n",
    "        f\"Expected window shape {(CFG.seq_len, CFG.n_features)}, \"\n",
    "        f\"got {window.shape}\"\n",
    "    )\n",
    "\n",
    "    window_batch = np.expand_dims(window, axis=0)  # (1, seq_len, n_features)\n",
    "    prob = model.predict(window_batch, verbose=0)[0, 0]\n",
    "    return float(prob)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 7. Точка входа\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Загружаем траектории ОДИН раз\n",
    "    trajectories = load_geolife_trajectories(CFG.geolife_root)\n",
    "\n",
    "    # 2) Тренируем модель\n",
    "    model = train_model(trajectories)\n",
    "\n",
    "    # 3) Тестовое предсказание на рандомном окне (просто sanity-check)\n",
    "    example_window = np.random.rand(CFG.seq_len, CFG.n_features).astype(\"float32\")\n",
    "    prob = predict_one_window(model, example_window)\n",
    "    print(\n",
    "        f\"Intrusion probability in next {CFG.horizon_steps} steps (random window): {prob:.3f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
